# -*- coding: utf-8 -*-
"""DS2010_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qJCtTXngHOdSNM3Da5hJFEYLaYG-7cwU
"""

import matplotlib.pyplot as plt
plt.style.use('ggplot')
import pandas as pd
from sklearn import tree
import numpy as np
from sklearn.svm import SVC 
from sklearn.metrics import precision_recall_fscore_support, classification_report
from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix
from sklearn.model_selection import train_test_split as tts
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
import seaborn as sns

dataset = pd.read_csv('./survey lung cancer.csv')


dataset

dataset['GENDER'] = dataset['GENDER'].map({'M':1,'F':0})
print(dataset['GENDER'])

dataset['LUNG_CANCER'] = dataset['LUNG_CANCER'].map({'YES':1,'NO':0})

dataset.plot()

con_col = ['AGE']
cat_col=[]
for i in dataset.columns:
    if i!='AGE':
        cat_col.append(i)
import warnings
warnings.filterwarnings('ignore')
fig,ax = plt.subplots(1,3,figsize=(20,6))
sns.distplot(dataset['AGE'],ax=ax[0])
sns.histplot(data =dataset,x='AGE',ax=ax[1],hue='LUNG_CANCER',kde=True)
sns.boxplot(x=dataset['LUNG_CANCER'],y=dataset['AGE'],ax=ax[2])
plt.suptitle("Visualizing AGE column",size=20)
plt.show()

X = dataset.iloc[:,:-1]
y = dataset['LUNG_CANCER']

for i in X.columns[2:]:
    temp=[]
    for j in X[i]:
        temp.append(j-1)
    X[i]=temp
X.head()

X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=123)

X_train.shape, X_test.shape

scaler = StandardScaler()
X_scaler = scaler.fit(X_train)
X_train['AGE'] = X_scaler.transform(X_train)
X_test['AGE'] = X_scaler.transform(X_test)

kernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']#A function which returns the corresponding SVC model

def getClassifier(ktype):
    if ktype == 0:
        # Polynomial kernal
        return SVC(kernel='poly', degree=8, gamma="auto")
    elif ktype == 1:
        # Radial Basis Function kernal
        return SVC(kernel='rbf', gamma="auto")
    elif ktype == 2:
        # Sigmoid kernal
        return SVC(kernel='sigmoid', gamma="auto")
    elif ktype == 3:
        # Linear kernal
        return SVC(kernel='linear', gamma="auto")

for i in range(4):
    # Separate data into test and training sets
    svclassifier = getClassifier(i) 
    svclassifier.fit(X_train, y_train)# Make prediction
    y_pred = svclassifier.predict(X_test)# Evaluate our model
    print("Evaluation:", kernels[i], "kernel")
    print(classification_report(y_test,y_pred))
    print(accuracy_score(y_test, y_pred), kernels[i])

#KNN
best_accuracy = 0
best_k = 1
models = []
for k in range(24):
  knn_model = KNeighborsClassifier(n_neighbors = k+1)
  knn_model = knn_model.fit(X_train, y_train)
  y_pred = knn_model.predict(X_test)# Evaluate our model
  accuracy = accuracy_score(y_test, y_pred)
  models.append(accuracy)
  if accuracy > best_accuracy:
    best_accuracy = accuracy
    best_k = k+1

knn_model = KNeighborsClassifier(n_neighbors = best_k)
knn_model = knn_model.fit(X_train, y_train)
y_pred = knn_model.predict(X_test)
print("Evaluation:", "KNN,", "k =", best_k)
print(classification_report(y_test,y_pred))

neighbors = range(1,25)
plt.title('KNN test accuracy vs. number of neighbors')
plt.plot(neighbors, models, label='Testing Accuracy')
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')
plt.show()

model_accuracy_score = accuracy_score(y_test, y_pred)
model_accuracy_score

# Decision Trees:
dt1_model = tree.DecisionTreeClassifier(random_state=1)
dt1_model = dt1_model.fit(X_train, y_train)
y_pred = dt1_model.predict(X_test)
print("Evaluation:", "Decision Trees")
print(classification_report(y_test,y_pred))

model_accuracy_score_dt = accuracy_score(y_test, y_pred)
model_accuracy_score_dt

# Random Forests:

rf1_model = RandomForestClassifier(n_estimators=50, random_state=1)
rf1_model = rf1_model.fit(X_train, y_train)
y_pred = rf1_model.predict(X_test)
print("Evaluation:", "Random Forests")
print(classification_report(y_test,y_pred))

model_accuracy_score_rf = accuracy_score(y_test, y_pred)
model_accuracy_score_rf

learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]

for learning_rate in learning_rates:
    svm_model = svm.SVC(kernel='linear', random_state=1, C=learning_rate)
    # Fit the model
    svm_model.fit(X_train, y_train)
    print("Learning rate: ", learning_rate)
    # Score the model
    print("Accuracy score (training): {0:.3f}".format(
        svm_model.score(
            X_train,
            y_train)))
    print("Accuracy score (validation): {0:.3f}".format(
        svm_model.score(
            X_test,
            y_test)))

plt.figure(figsize=(8,8))
sns.heatmap(dataset.corr(),annot=True,linewidth=0.5,fmt='0.2f')

#Confusion Matrix for SVC

import numpy

actual = y_test
predicted = y_pred

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(actual, predicted)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

import matplotlib.pyplot as plt

cm_display.plot()
plt.show()

model_importances = pd.DataFrame(rf1_model.feature_importances_, index = X_train.columns, columns=['Importance']).sort_values('Importance', ascending=False)
model_importances

importances = rf1_model.feature_importances_
feature_names = [f"feature {i}" for i in range(X.shape[1])]
forest_importances = pd.Series(importances, index=feature_names)
std = np.std([tree.feature_importances_ for tree in rf1_model.estimators_], axis=0)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

